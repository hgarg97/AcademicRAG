# **AcademicRAG - Vector Store Implementation ğŸ“šğŸ”**

This implementation of **AcademicRAG** leverages **Astra DB (Serverless Cassandra with Vector Search)** and **LLMs (OpenAI/Groq LLAMA 3.3)** to create an intelligent research assistant.

---

## **ğŸ—‚ Folder Structure**

- `create_vector_store.py` â†’ Extracts text, generates embeddings, and stores in Astra DB.
- `app.py` â†’ Streamlit chatbot retrieving documents via Astra DB.
- `requirements.txt` â†’ Dependencies for this implementation.

---

## **ğŸ¤ Setup Instructions**

### **1ï¸âƒ£ Create a Virtual Environment**

```bash
conda create -p venv python==3.10
conda activate venv/
```

### **2ï¸âƒ£ Install Dependencies**

```bash
pip install -r requirements.txt
```

### **3ï¸âƒ£ Configure Environment Variables**

Create a `.env` file with:

```ini
ASTRA_DB_APPLICATION_TOKEN=<your_astra_db_token>
ASTRA_DB_ID=<your_astra_db_id>
OPENAI_API_KEY=<your_openai_api_key>
GROQ_API_KEY=<your_groq_api_key>
```

Or set variables manually:

```bash
export ASTRA_DB_APPLICATION_TOKEN="your_astra_db_token"
export OPENAI_API_KEY="your_openai_api_key"
```

---

## **ğŸš€ Running the Chatbot**

### **1ï¸âƒ£ Store Research Paper Embeddings**

```bash
python create_vector_store.py
```

### **2ï¸âƒ£ Launch the Chatbot**

```bash
streamlit run app.py
```

---

## **ğŸ”¬ Workflow Overview**

1ï¸âƒ£ **Extract text from PDFs using PyMuPDF.**  
2ï¸âƒ£ **Split text into chunks (800 tokens, 200 overlap) for retrieval.**  
3ï¸âƒ£ **Generate embeddings using `all-MiniLM-L6-v2`.**  
4ï¸âƒ£ **Store embeddings in AstraDB for vector-based search.**  
5ï¸âƒ£ **Retrieve top 4 relevant documents via similarity search.**  
6ï¸âƒ£ **Generate responses using LLAMA 3.3 (Groq API).**  
7ï¸âƒ£ **Display responses in a Streamlit chatbot.**

---

## **ğŸ“Œ Future Enhancements**

- Experiment with better chunking methods.
- Optimize retrieval efficiency.
- Compare different embedding models.

ğŸ’¡ **Part of a broader RAG experimentationâ€”stay tuned for more implementations!** ğŸš€
