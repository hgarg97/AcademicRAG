# ğŸ§  Retrieval-Augmented Generation (RAG) Chatbot for Academic PDFs

This project is a fully modular, class-based **RAG (Retrieval-Augmented Generation) system** that allows you to query a collection of academic **PDFs** and get contextual answers using **LLaMA 3.2 via Ollama**.

It integrates:

- Document preprocessing and intelligent **chunking**
- **Dense vector search (FAISS)** for semantic retrieval
- Optional **sparse keyword search (BM25)** for exact match queries
- A **Streamlit chatbot** interface powered by LLMs

---

## ğŸš¦ End-to-End Workflow Overview

```mermaid
graph TD;
    A[PDFs] --> B[Chunking: chunking.py];
    B --> C[Vector Embeddings: embedding.py];
    B --> D[BM25 Indexing (optional): bm25.py];
    C --> E[FAISS Index];
    D --> E2[BM25 Index];
    E --> F[Chatbot Query: RAG.py];
    E2 --> F;
    F --> G[Answer via LLaMA 3.2 (Ollama)];
```

---

## ğŸ“‚ Folder Structure & Key Files

| File            | Purpose                                          |
| --------------- | ------------------------------------------------ |
| `chunking.py`   | Extract and intelligently chunk PDF text         |
| `embedding.py`  | Create vector embeddings using MiniLM            |
| `bm25.py`       | Create a sparse keyword search index             |
| `RAG.py`        | RAG chatbot logic with retriever + LLM           |
| `config.py`     | Centralized config for paths and model names     |
| `main.py`       | Unified CLI for preprocessing and chatbot launch |
| `vector_store/` | Stores FAISS + BM25 indexes and metadata         |

---

## ğŸš€ How to Use

### âœ… 1. Install Dependencies

```bash
pip install -r requirements.txt
```

---

### ğŸ§± 2. Preprocess the PDFs

This step:

- Reads all PDFs
- Chunks their text
- Generates embeddings for semantic retrieval (FAISS)
- (Optional) Creates BM25 index for exact keyword search

#### â¤ Run FAISS-only pipeline:

```bash
python main.py --mode preprocess
```

#### â¤ Run FAISS + BM25 indexing:

```bash
python main.py --mode preprocess --use_bm25
```

---

### ğŸ’¬ 3. Launch the Chatbot

You can choose **which retrieval mode to use**:

| Mode              | Description                                                         |
| ----------------- | ------------------------------------------------------------------- |
| `faiss` (default) | Uses dense semantic retrieval â€” best for natural language queries   |
| `bm25`            | Uses keyword-based search â€” good for exact matches or rare terms    |
| `hybrid`          | Combines both FAISS and BM25 â€” best for balanced recall & precision |

#### â¤ Launch (default = FAISS):

```bash
streamlit run main.py -- --mode chatbot
```

#### â¤ Use BM25 only:

```bash
streamlit run main.py -- --mode chatbot --retriever bm25
```

#### â¤ Use hybrid (FAISS + BM25):

```bash
streamlit run main.py -- --mode chatbot --retriever hybrid
```

---

## ğŸ§  How Retrieval Works

### âœ… FAISS (Dense Semantic Retrieval)

- Uses `all-MiniLM-L6-v2` via `sentence-transformers`
- Captures meaning beyond keywords
- Great for paraphrased or long-form questions
- Leverages GPU if available

### âœ… BM25 (Sparse Keyword Retrieval)

- Based on token overlap + frequency
- Great for matching **exact phrases**, **chemical names**, or **IDs**
- Especially helpful for niche scientific jargon

### âœ… Hybrid

- Retrieves top-k from both BM25 + FAISS
- De-duplicates and merges
- Ensures **strong recall** and **semantic depth**

---

## ğŸ”§ Configuration

All file paths and model names are stored in:

```python
config.py
```

You can change:

- `RAW_PDF_DIR` â€” path to your academic PDFs
- `LLM_MODEL_NAME` â€” Ollama model to run (e.g. `"llama3.2:latest"`)
- `EMBEDDING_MODEL_NAME` â€” transformer model for embeddings

---

## ğŸ” Benefits of the System

- âš™ï¸ Fully modular (chunking, embedding, BM25, chatbot)
- ğŸ§  LLM-powered contextual answers
- ğŸ”Œ Easily swappable retrievers (FAISS, BM25, Hybrid)
- ğŸš€ GPU-compatible for faster embeddings
- ğŸ–¼ï¸ Interactive UI with Streamlit
- ğŸ”„ Automatic Ollama startup if not already running
- ğŸ”“ Designed to scale and integrate future enhancements (e.g. GraphRAG)

---

## ğŸ™Œ Credits & Tools Used

- FAISS â€” Dense vector search
- RankBM25 â€” Sparse keyword search
- LangChain â€” LLM orchestration
- Ollama â€” Local LLM serving
- SentenceTransformers â€” Embeddings

---

Built for deep, document-grounded academic question answering.  
_Query smarter, not harder_ ğŸ¤–ğŸ“˜âœ¨
